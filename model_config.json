{
  "primary_model": {
    "model_name": "llama-3.1-70b-instruct",
    "model_type": "gguf",
    "model_path": "models/llama-3.1-70b-instruct.Q6_K.gguf",
    "model_url": "https://huggingface.co/TheBloke/Llama-3.1-70B-Instruct-GGUF/resolve/main/llama-3.1-70b-instruct.Q6_K.gguf",
    "device": "cuda",
    "n_gpu_layers": -1,
    "n_ctx": 8192,
    "max_tokens": 4096,
    "max_batch_size": 8,
    "tensor_split": "0.99,0.01",
    "threads": 12,
    "use_mlock": true,
    "n_batch": 512,
    "stream_output": true,
    "download_if_missing": true,
    "max_concurrent_requests": 12
  },
  "embedding_model": {
    "model_name": "nomic-ai/nomic-embed-text-v1",
    "model_type": "huggingface",
    "model_path": "models/nomic-embed-text-v1",
    "device": "cuda",
    "max_batch_size": 64,
    "max_concurrent_requests": 24,
    "download_if_missing": true
  },
  "classifier_model": {
    "model_name": "mistral-7b-instruct-v0.2",
    "model_type": "gguf",
    "model_path": "models/mistral-7b-instruct-v0.2.Q4_K_M.gguf",
    "model_url": "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf",
    "device": "cuda",
    "n_gpu_layers": -1,
    "n_ctx": 4096,
    "max_tokens": 1024,
    "max_batch_size": 16,
    "threads": 6,
    "use_mlock": true,
    "n_batch": 256,
    "stream_output": false,
    "download_if_missing": true,
    "max_concurrent_requests": 16
  }
}
